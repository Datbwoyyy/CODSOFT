{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 5186786,
          "sourceType": "datasetVersion",
          "datasetId": 3015609
        }
      ],
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "sabahesaraki_2017_2017_path = kagglehub.dataset_download('sabahesaraki/2017-2017')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "kSxBZQfW4uI3"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "ZpjE7K-Z4uI9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow keras opencv-python matplotlib transformers"
      ],
      "metadata": {
        "id": "jHhVLDRe4wli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ImAge Feature Extraction using Pre-Trained CNN:\n",
        "\n",
        "\n",
        "*I will be making Use  Of RESNET50(50 Layers)"
      ],
      "metadata": {
        "id": "aTeQqlQH5JVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "# Paths to the image and annotations\n",
        "image_folder =\n",
        "annotations_path =\n",
        "\n",
        "# Loading captions JSON File\n",
        "with open(annotations_path, 'r') as f:\n",
        "  annotations =json.load(f)\n",
        "\n",
        "# Creating a dictionary to map image IDs to captions\n",
        "captions_dict = {}\n",
        "for annotation in annotations:\n",
        "  image_id = annotation['image_id']\n",
        "  caption = annotation['caption']\n",
        "  if image_id not in captions_dict:\n",
        "    captions_dict[image_id]= []\n",
        "  captions_dict[image_id].append(caption)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bt9UNmKe5Fmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Features from All Images in the folder Using ResNet50\n"
      ],
      "metadata": {
        "id": "CD8l2B3vmZuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading the Resnet50 without the top layer for feature extraction\n",
        "base_model = ResNet50(weights='imagenet', include_top='False', pooling='avg')\n",
        "model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
        "\n",
        "def extract_features(img_path):\n",
        "  img = image.load_img(img_path, target_size=(224, 224)) ## Resizing to 224 X 224\n",
        "  img_array = image.img_to_array(img)\n",
        "  img_array = np.expand_dims(img_array, axis=0)\n",
        "  img_array = preprocess_input(img_array) ## Preprocessing the image in ResNet50\n",
        "  features = model.predict(img_array,verbose=0)\n",
        "  return features\n",
        "\n",
        "\n",
        "## Extract Features Of all images\n",
        "image_features= {}\n",
        "for img_file in os.listdir(image_folder):\n",
        "  img_id = int(img_file.split('.')[0]) # Get immage ID from file name\n",
        "  img_path = os.path.join(image_folder, img_file)\n",
        "  features = extract_features(img_path)\n",
        "  image_features[img_id] = features\n",
        "\n",
        "## Check extracted features\n",
        "print(f\"Extracted features for {len(image_features)} images.\")\n"
      ],
      "metadata": {
        "id": "xzbM0aKfmVPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4: Prepare Captions For Training\n",
        "Tokenizing the captions so as to feed them into the Long short Term memory(LSTM) or Transformer\n"
      ],
      "metadata": {
        "id": "xVEkWZzipxBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "## Get all captions\n",
        "all_captions = [ caption for captions in captions_dict.values() for caption in captions]\n",
        "\n",
        "## Tokenize Captions\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = lent(tokenizer.word_index)+ 1\n",
        "\n",
        "# Convert Captions to Sequences\n",
        "max_length = max(len(caption.split())for caption in all_captions)\n",
        "sequences = tokenizer.texts_to_sequences(all_captions)\n",
        "padded_sequences =pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "print(f\"Vocabulary_size: {voca_size}\")\n",
        "print(f\"Max Caption length: {max_length}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ayUebkOBqBpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5 :Building the Image Captioning Model (CNN + LSTM)\n",
        "Building a simple CNN And LOng Short term Memory Model for image Captioning\n"
      ],
      "metadata": {
        "id": "lWZ4M19Dsp9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Feature input (image features from ResNet50)\n",
        "image_input = Input(shape=(2048,))\n",
        "image_dense = Dense(256, activation='relu')(image_input)\n",
        "\n",
        "## Captioning Input\n",
        "caption_input = Input(shape=(max_length,))\n",
        "caption_emb = Embedding(vocab_size, 256)(caption_input)\n",
        "caption_lstm = LSTM(256)(caption_emb)\n",
        "\n",
        "## Combine image features and captions\n",
        "decoder = Concatenate()([image_dense, caption_lstm])\n",
        "output = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "# Build the Model\n",
        "model = Model(inputs = [image_input, caption_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "Ayt4pWqHsnl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TRaining batches of image features and Captions"
      ],
      "metadata": {
        "id": "FaswuWoRwGB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for img_id captions in captions_dict.items():\n",
        "  image_feat = image_features[img_id]\n",
        "  for caption in captions:\n",
        "    seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "    for i in range(1, len(seq)):\n",
        "      input_seq, output_word = seq[:1], seq[i]\n",
        "      input_seq = pad_sequences([input_seq], maxlen=max_length)[0]\n",
        "      output_word = np.zeros(vocab_size)\n",
        "      output_word[seq[i]]=1\n",
        "      model.train_on_batch([image_feat, input_seq], output_word)\n",
        ""
      ],
      "metadata": {
        "id": "SuPW8NYzvwQc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating the Caption For a New Image"
      ],
      "metadata": {
        "id": "eqG-lWek0AcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(image_features, tokenizer, max_length):\n",
        "  input_text = '<start>'\n",
        "  for i in range(max_length):\n",
        "    seq = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    seq = pad_sequences([seq], maxlen=max_length)\n",
        "    prediction = model.predict([image_features,seq], verbose=0)\n",
        "    predicted_word = tokenizer.index_word[np.argmax(prediction)]\n",
        "    input_text += ' ' + predicted_word\n",
        "    if predicted_word == '<end>':\n",
        "      break\n",
        "return input_text\n"
      ],
      "metadata": {
        "id": "cM0ekfAlv5HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing The Image Caption Model"
      ],
      "metadata": {
        "id": "Ozdoyctez5Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_id= list(image_features.keys())[0]\n",
        "print(generate_caption(image_features[image_id],tokenizer, max_length))"
      ],
      "metadata": {
        "id": "0gMoG_sFz29c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}